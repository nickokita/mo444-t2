from utils import *
from models.logistic_regression import LogisticRegression
from models.softmax_regression import SoftmaxRegression
import sys

def run_softmax():
    ret = load_data(sys.argv[1], int(sys.argv[2]))
    features = decision_boundaries_pol(ret[0], 1)
    target = ret[1]

    if len(sys.argv) == 6:
        _model = sys.argv[5]
    else:
        _model = []

    LR = SoftmaxRegression(features, target, 0.01, classes=unique_classes(target),
                           epoch=int(sys.argv[4]), model=_model, mini_batch_size=128)

    print(LR.softmax_regression())
    print(LR.get_cost())
    # LR.update_model_sgd()
    # LR.update_model_batch()

    #print(LR.get_model())
    #print(LR.get_cost())
    #print(LR.get_predict())

def main():
    if len(sys.argv) < 4:
        print("Usage: python main.py <data> <targetcol> <target> <epoch>")
        exit(1)

    ret = logistic_load(sys.argv[1], int(sys.argv[2]), int(sys.argv[3]))
    #ret = load_data(sys.argv[1], int(sys.argv[2]))
    features = decision_boundaries_pol(ret[0], 2)
    target = ret[1]
    print(target)

    if len(sys.argv) == 6:
        _model = sys.argv[5]
    else:
        _model = []
        #_model = [0.99, 1.0, 1.0, 1.0, 1.0, 0.96, 1.0, 1.0, 0.98, 1.0, 0.59, 1.0, 1.0, 1.0, -0.8500000000000001, 0.28000000000000014, 1.0, -0.3700000000000001, -1.1600000000000001, 0.9299999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -0.9299999999999999, -1.0899999999999999, 16.92, -0.9299999999999999, 0.0, 1.0, -1.5500000000000003, -0.79, 1.0, -0.3700000000000001, 1.0, 1.0, 0.16999999999999993, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 0.95, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -0.69, 0.94, 1.0, -1.0, -0.51, 1.0, 1.0, 1.0, 0.73, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.3900000000000001, 1.0, -0.69, 7.000000000000001, 1.0, 0.19999999999999996, 1.0, 1.0, -0.3500000000000001, -8.28, 1.0, 0.98, -0.30000000000000004, 1.0, 1.0, -1.02, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 1.0, 1.0, 0.08999999999999997, -1.2000000000000002, -5.03, 1.0, -1.4100000000000001, -0.73, -3.56, 1.0, 1.0, 1.0, 1.0, 0.44999999999999996, 1.0, 1.0, 1.0, 0.51, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6799999999999999, 1.0, 1.0, -1.1800000000000002, 1.0, -1.38, 1.0, 0.95, 1.0, -0.9299999999999999, 1.0, 1.0, -0.15999999999999992, 1.0, -5.079999999999999, -0.020000000000000018, 1.6400000000000006, 1.0, -0.6400000000000001, 1.0, -0.6699999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28, 1.0, 1.0, 1.0, -0.10000000000000009, 1.0, -0.71, -0.8700000000000001, 0.19999999999999996, -0.3400000000000001, 0.83, -1.4300000000000002, 1.0, -0.5, -1.21, -4.4, 0.24, 1.0, 1.0, 1.0, -0.5900000000000001, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.02, 1.0, -0.5800000000000001, -0.77, 1.0, -0.1100000000000001, 1.0, -0.3500000000000001, 0.040000000000000036, -0.99, 19.72, -0.41999999999999993, 1.0, -0.99, -1.21, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -0.6699999999999999, 1.0, -0.98, 0.08999999999999997, -1.2000000000000002, -0.81, 0.79, 1.0, -0.79, 1.0, -1.31, -7.52, -0.44999999999999996, 1.0, 0.76, -1.2800000000000002, 1.0, 1.0, 0.95, -1.0699999999999998, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.79, 0.0, 1.0, 1.0, -0.41999999999999993, 0.99, -0.6200000000000001, -1.3900000000000001, -1.23, -6.5, 0.14, 1.0, -0.9100000000000001, -0.9199999999999999, 1.0, 1.0, 0.030000000000000027, -0.8600000000000001, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -16.1, 1.0, 1.0, 1.0, 1.0, -3.6500000000000004, -0.48, 1.0, 0.0, 0.42000000000000004, -0.8300000000000001, -0.6799999999999999, 1.0, -5.0, -0.4099999999999999, 0.71, -1.1400000000000001, -0.45999999999999996, -1.29, 1.0, 1.0, 0.51, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0899999999999999, -0.29000000000000004, 1.0, 1.0, -0.95, 1.0, 0.96, 1.0, -0.27, 18.680000000000003, -0.45999999999999996, -0.73, 0.77, 0.0, 0.72, -0.9299999999999999, 0.13, -3.8600000000000003, -1.2600000000000002, -0.6799999999999999, 1.0, -0.78, 1.0, 0.010000000000000009, 1.0, 1.0, 1.0, 1.0, 1.0, 0.53, 1.0, -0.8300000000000001, -0.30000000000000004, 0.18999999999999995, -0.6799999999999999, -0.5, -0.6500000000000001, -0.73, -0.95, 0.25, 0.85, -1.3599999999999999, 0.84, -1.8399999999999999, -0.6100000000000001, -1.04, -1.33, -0.8700000000000001, 0.95, -0.030000000000000027, -0.95, -0.30000000000000004, 1.0, 1.0, 1.0, 1.0, -0.010000000000000009, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.12, -5.57, 1.0, -0.52, -1.2000000000000002, 1.0, -1.44, 1.0, -1.2600000000000002, -1.35, 1.0, 1.0, -1.17, 1.0, 1.0, 0.19999999999999996, 1.0, 1.0, 1.0, 1.0, 1.0, 0.18999999999999995, 1.0, 1.0, 1.0, -0.19999999999999996, -0.19999999999999996, -0.8200000000000001, 0.21999999999999997, 0.54, -13.399999999999999, -0.71, -0.8800000000000001, -1.35, -1.29, -3.34, 0.6799999999999997, -0.1200000000000001, -0.81, -1.0699999999999998, 0.99, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 0.15000000000000002, 1.0, -0.8600000000000001, -0.49, -0.1200000000000001, -1.1, -0.07000000000000006, 0.98, -1.1099999999999999, -0.15999999999999992, -0.3600000000000001, 0.53, -4.6, 1.0, 1.0, 1.0, -0.78, -0.98, -0.4099999999999999, -1.3599999999999999, 1.0, -0.1100000000000001, 0.22999999999999998, -1.5500000000000003, 1.0, -0.9199999999999999, 1.0, 1.0, 0.79, 1.0, 0.10999999999999999, -0.8600000000000001, 1.0, -1.3399999999999999, -0.96, 18.12, -0.19999999999999996, -1.2200000000000002, 0.09999999999999998, -0.75, -1.0300000000000002, -1.0899999999999999, -1.3599999999999999, -1.1, -1.38, 1.0, 0.71, 1.0, 1.0, 0.77, 0.040000000000000036, 1.0, 1.0, 0.52, 1.0, -1.4, 1.0, -1.12, 1.0, -0.9199999999999999, 0.6699999999999999, -3.0500000000000003, 19.400000000000002, 0.39, -0.96, -1.06, 15.400000000000002, 0.8200000000000001, -1.02, 18.92, 0.040000000000000036, 1.0, 1.0, -1.0899999999999999, -1.13, 1.0, 1.0, 1.0, 1.0, 1.0, -0.6000000000000001, 1.0, 1.0, 1.0, 1.0, 0.94, -1.48, -0.73, -1.35, 1.0, 5.08, 0.04999999999999993, -1.13, 1.0, -1.0499999999999998, 1.0, -10.16, -10.480000000000002, -0.76, -0.30000000000000004, 1.0, -0.45999999999999996, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.46, -1.2400000000000002, 0.86, 1.0, -1.25, 0.73, 0.64, -0.81, -15.47, -0.17999999999999994, -1.3199999999999998, 7.600000000000001, -1.2400000000000002, 0.91, -1.08, -0.8600000000000001, -0.6400000000000001, -0.30000000000000004, 1.0, -1.0899999999999999, 1.0, 0.62, 1.0, 0.91, 1.0, 1.0, -2.7, 1.0, -1.5500000000000003, 0.9299999999999999, -1.46, -0.1100000000000001, -1.25, -1.33, 0.75, 19.16, -0.3800000000000001, -0.72, 0.6799999999999999, -2.1, 0.38, -0.49, -1.23, -0.9000000000000001, 1.0, -0.8200000000000001, -0.81, 0.53, 1.0, 1.0, 1.0, 0.99, -7.64, 0.14, 1.0, -3.0500000000000003, 1.0, 1.0, -1.2200000000000002, -1.1, -2.5, -1.1099999999999999, 1.0, -1.17, -1.04, -1.06, -1.0300000000000002, -0.6100000000000001, 0.5800000000000001, -1.2200000000000002, -1.13, -0.79, -0.81, 1.0, 1.0, 1.0, -0.76, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -18.62, -1.1600000000000001, 0.94, -0.3900000000000001, -0.09000000000000008, 1.0, -0.6600000000000001, -0.6699999999999999, 1.0, 0.29999999999999993, 1.0, 0.29000000000000004, 1.0, -0.15999999999999992, 1.0, 1.0, 1.0, 1.0, -0.73, 1.0, 1.0, 1.0, 1.0, 1.0, -0.27, 1.0, -1.27, 1.0, -1.35, 0.18999999999999995, -1.25, -1.0300000000000002, 1.0, 6.16, 18.44, -1.1800000000000002, -0.96, -1.1400000000000001, -0.5, -0.79, -0.3400000000000001, 14.08, 0.43999999999999995, 1.0, 0.35, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.62, 1.0, -3.7700000000000005, -1.37, 1.0, -0.99, 1.0, -1.17, -0.10000000000000009, -0.5, -3.38, -0.6600000000000001, -1.31, 0.030000000000000027, 0.6599999999999999, 1.0, -8.85, -0.4099999999999999, -0.79, 1.0, 1.0, 1.0, 1.0, 1.0, -0.29000000000000004, 1.0, 1.0, 1.08, 1.0, 1.0, 1.0, 0.49, 1.0, -0.3800000000000001, -1.23, 1.0, 0.51, 0.25, -0.76, 1.0, 0.99, 0.29999999999999993, -0.8600000000000001, 1.0, 1.0, -0.73, 1.0, -0.43999999999999995, 1.0, 5.3999999999999995, 1.0, 1.0, 1.0, 1.0, 1.0, -0.030000000000000027, -1.2400000000000002, 1.0, 1.0, -0.18999999999999995, -0.21999999999999997, -0.9100000000000001, -1.1800000000000002, -3.58, 17.72, -0.32000000000000006, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.97, 0.61, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -0.08000000000000007, -0.6699999999999999, 1.0, 1.0, 0.98, -1.0100000000000002, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1127.9600000000003, -271.25, 1.0, -423.36, 1.0, 0.9199999999999999, -3.41, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -173.24, 1.0, 1.0, 1.0, -560.69, 1.0, -236.63000000000002, -221.01, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -168.28, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -209.25, 1.0, -19.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -575.24, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -107.16, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1374.88, 548.5600000000001, -629.01, 1.0, -419.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -6.22, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 250.64000000000001, 1.0, 539.24, 1.0, 1.0, 1.0, 314.29, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -143.0, 389.09000000000003, 0.99, -4043.96, 1.0, -474.24, 3088.0, -483.0, -162.84, 5491.81, 1.0, -483.0, -577.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -3263.32, 1.0, 1.0, 1.0, 1.0, -352.44, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -427.49, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -399.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1050.76, 32.36, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1588.0, 1.0, 1.0, -66.24, 1.0, -242.36, -1865.24, 1844.1999999999998, 1.0, 1.0, 1.0, 1.0, -1655.2, -528.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -23.01, 1.0, 19.490000000000002, 1.0, 1.0, 1.0, -1477.52, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1567.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 335.89, 1.0, 1.0, 1.0, 1.0, 1.0, 3092.74, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, -352.44, 1.0, 1.0, 1.0, -71.25, 1.0, 241.25, 1.0, 1.0, -478.61, 3565.09, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 621.01, 1.0, 1.0, -206.36, 1.0, 1.0, 651.25, 1.0, 1.0, 1.0, -274.56, 1.0, 0.28, 1.0, 1.0, 1.0, 1.0, -583.82, 1.0, 1.0, -348.69, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -624.0, 1.0, -245.49, 1.0, 1.0, -1103.5, 1.0, 1.0, 1.0, -1865.24, -2397.05, 1.0, 1.0, 1.0, 1.0, -560.69, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 651.25, -1084.78, -298.29, 1.0, 1.0, 1.0, -2947.4900000000007, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -2499.47, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -415.16, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -107.0, 1.0, 1.0, 1.0, 1.0, 1.0, -294.84000000000003, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 109.16, 1.0, 1.0, 1.0, -8.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1847.32, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 3250.0, 1.0, 1.0, 1.0, 273.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -158.87, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96, 1.0, 1.0, 1.0, 1.0, 1.0, -2980.16, 1.0, 1.0, 1.0, 1.0, 1.0, -565.44, 1.0, 1.0, 0.6799999999999997, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -335.2, 1.0, 1.0, 1.0, 1.0, 1.0, -3744.4399999999996, 1.0, 1.0, 1.0, 1.0, 1.0, -509.76, 1.0, 1.0, -1124.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 4638.61, 1.0, 1.0, 1.0, -77.03, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -744.29, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -2351.2400000000002, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

    LR = LogisticRegression(features, target, 0.01, epoch=int(sys.argv[4]), model=_model, mini_batch_size=128, threshold=0.5)

    LR.update_model_mini_batch()
    #LR.update_model_sgd()
    #LR.update_model_batch()

    print(LR.get_model())
    print(LR.simplified_cost_function())
    print(LR.get_predict())

    #SKLR = SKLogisticRegression(features, target, 0.01, 0.5, 100)
    #SKLR.update_model_sgd()

    #print(SKLR.get_predict())
    print(target)

if __name__ == "__main__":
    run_softmax()
